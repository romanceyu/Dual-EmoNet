import os
import cv2
import torch
import numpy as np
import matplotlib.pyplot as plt
from torchvision import transforms
from PIL import Image
import matplotlib as mpl

# ==================== å­¦æœ¯å›¾è¡¨å…¨å±€é…ç½® ====================
plt.style.use('ggplot')  # æ›´å­¦æœ¯çš„ç½‘æ ¼æ ·å¼
mpl.rcParams.update({
    'font.family': 'DejaVu Sans',  # å¼€æºæ— è¡¬çº¿å­—ä½“
    'axes.titlesize': 16,  # IEEE Trans æ¨èæ ‡é¢˜å­—å·
    'axes.labelsize': 14,  # åæ ‡è½´æ ‡ç­¾å­—å·
    'xtick.labelsize': 12,
    'ytick.labelsize': 12,
    'figure.dpi': 300,  # äº¤äº’æ˜¾ç¤ºæ¸…æ™°åº¦
    'savefig.dpi': 600,  # å°åˆ·çº§åˆ†è¾¨ç‡
    'image.cmap': 'jet',  # è‰²ç›²å‹å¥½è‰²ç³»
    'legend.frameon': True,  # å›¾ä¾‹è¾¹æ¡†
    'figure.constrained_layout.use': True,
    'figure.figsize': (18, 6)  # æ ‡å‡†ä¸‰æ å¸ƒå±€å°ºå¯¸
})


class GradCAMGenerator:
    """é¢å‘å­¦æœ¯å‡ºç‰ˆçš„ Grad-CAM å¯è§†åŒ–å·¥å…·

    ä¸»è¦ä¼˜åŒ–ç‰¹æ€§:
    - æ”¯æŒæ‰¹é‡å¤„ç† (å•å›¾/ç›®å½•)
    - è‡ªé€‚åº”è®¾å¤‡é€‰æ‹© (CUDA/MPS/CPU)
    - å¯é…ç½®çš„å­¦æœ¯å›¾è¡¨æ ·å¼
    - å¼‚å¸¸å¤„ç†æœºåˆ¶
    """

    def __init__(self, model_dir, model_file):
        # è®¾å¤‡é…ç½®
        self.device = self._get_device()
        print(f"âš™ï¸ è¿è¡Œè®¾å¤‡: {self.device}")

        # æ¨¡å‹åˆå§‹åŒ–
        self.model = self._load_model(model_dir, model_file)
        self.hook = None

        # å›¾åƒé¢„å¤„ç†æµæ°´çº¿
        self.preprocess = transforms.Compose([
            transforms.Resize((64, 64)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
        ])

    class Hook:
        """æ¢¯åº¦æ•è·å·¥å…· (æ”¯æŒå¤šæ¨¡æ€ç‰¹å¾æå–)"""

        def __init__(self):
            self.forward_out = None
            self.backward_out = None

        def register(self, module):
            def forward_hook(_, __, output):
                self.forward_out = output.detach()

            def backward_hook(_, grad_input, grad_output):
                self.backward_out = grad_output[0].detach()

            module.register_forward_hook(forward_hook)
            module.register_full_backward_hook(backward_hook)

    def _get_device(self):
        """è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è®¡ç®—è®¾å¤‡"""
        if torch.cuda.is_available():
            return torch.device("cuda")
        elif torch.backends.mps.is_available():
            return torch.device("mps")
        else:
            return torch.device("cpu")

    def _load_model(self, model_dir, model_file):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ (å«å®Œæ•´æ€§æ ¡éªŒ)"""
        from approach.DA_EmoNet2 import ImprovedResEmoteNet

        model_path = os.path.join(model_dir, model_file)
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"âŒ æ¨¡å‹æ–‡ä»¶ç¼ºå¤±: {model_path}")

        print(f"âœ… æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        model = ImprovedResEmoteNet().to(self.device)
        checkpoint = torch.load(model_path, map_location=self.device)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        return model

    def _process_image(self, image_path):
        """å›¾åƒé¢„å¤„ç†æµæ°´çº¿ (å«å¼‚å¸¸å¤„ç†)"""
        try:
            img = Image.open(image_path).convert('RGB')
            original_img = np.array(img)
            img_tensor = self.preprocess(img).unsqueeze(0).to(self.device)
            return original_img, img_tensor
        except Exception as e:
            raise RuntimeError(f"å›¾åƒå¤„ç†å¤±è´¥: {image_path} | é”™è¯¯: {str(e)}")

    def generate_cam(self, image_path):
        """ç”Ÿæˆ Grad-CAM çƒ­åŠ›å›¾"""
        try:
            # æ³¨å†Œé’©å­åˆ°æœ€åä¸€ä¸ªå·ç§¯å±‚
            self.hook = self.Hook()
            self.hook.register(self.model.conv3)

            # å›¾åƒé¢„å¤„ç†
            original_img, img_tensor = self._process_image(image_path)

            # å‰å‘ä¼ æ’­
            logits = self.model(img_tensor)
            probabilities = torch.nn.functional.softmax(logits, dim=1)
            pred_class = torch.argmax(probabilities).item()

            # åå‘ä¼ æ’­
            self.model.zero_grad()
            one_hot = torch.zeros_like(logits)
            one_hot[0][pred_class] = 1
            logits.backward(gradient=one_hot, retain_graph=True)

            # è®¡ç®— CAM
            weights = torch.mean(self.hook.backward_out, dim=[2, 3], keepdim=True)
            cam = torch.sum(weights * self.hook.forward_out, dim=1, keepdim=True)
            cam = torch.relu(cam).squeeze().cpu().numpy()

            # åå¤„ç†
            cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)
            cam = cv2.resize(cam, (original_img.shape[1], original_img.shape[0]))
            return original_img, cam, pred_class

        except Exception as e:
            print(f"ğŸ”¥ Grad-CAM ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise

    def visualize(self, original_img, cam, output_path, filename):
        """ä¼˜åŒ–åçš„å­¦æœ¯çº§å åŠ æ•ˆæœ"""
        try:
            # å¢å¼ºCAMå¯¹æ¯”åº¦ (éçº¿æ€§ä¼½é©¬æ ¡æ­£)
            cam_enhanced = np.power(cam, 0.6)  # æŒ‡æ•°å°äº1ä»¥å¢å¼ºé«˜å€¼åŒºåŸŸ

            # ç”Ÿæˆé«˜å¯¹æ¯”åº¦çƒ­åŠ›å›¾ (æ”¹ç”¨JETè‰²å›¾)
            heatmap = cv2.applyColorMap(np.uint8(255 * cam_enhanced), cv2.COLORMAP_VIRIDIS)

            # è°ƒæ•´å åŠ æ¯”ä¾‹ (åŸå›¾é€æ˜åº¦é™ä½)
            superimposed = cv2.addWeighted(
                cv2.cvtColor(original_img, cv2.COLOR_RGB2BGR), 0.5,  # åŸå›¾å æ¯”30%
                heatmap, 0.5,  # çƒ­åŠ›å›¾å æ¯”70%
                0  # äº®åº¦è°ƒèŠ‚
            )

            # åå¤„ç†å¢å¼º
            superimposed = cv2.cvtColor(superimposed, cv2.COLOR_BGR2RGB)
            lab = cv2.cvtColor(superimposed, cv2.COLOR_RGB2LAB)
            l, a, b = cv2.split(lab)
            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))  # è‡ªé€‚åº”å¯¹æ¯”åº¦å¢å¼º
            limg = cv2.merge([clahe.apply(l), a, b])
            enhanced_img = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)

            # åˆ›å»ºä¸“ä¸šå¯è§†åŒ–å¸ƒå±€
            fig = plt.figure(figsize=(15, 8))
            gs = fig.add_gridspec(2, 4, width_ratios=[1, 1, 1, 0.05])

            # å­å›¾1: åŸå§‹å›¾åƒ
            ax1 = fig.add_subplot(gs[:, 0])
            ax1.imshow(original_img)
            ax1.set_title('(a) Input Image', fontsize=14, y=0.95,
                          bbox=dict(facecolor='white', alpha=0.8, edgecolor='none'))
            ax1.axis('off')

            # å­å›¾2: çº¯çƒ­åŠ›å›¾
            ax2 = fig.add_subplot(gs[:, 1])
            im = ax2.imshow(cam_enhanced, cmap='viridis')
            ax2.set_title('(b) Activation Map', fontsize=14, y=0.95,
                          bbox=dict(facecolor='white', alpha=0.8, edgecolor='none'))
            ax2.axis('off')

            # å­å›¾3: ä¼˜åŒ–å åŠ æ•ˆæœ
            ax3 = fig.add_subplot(gs[:, 2])
            ax3.imshow(enhanced_img)
            ax3.set_title('(c) Enhanced Overlay', fontsize=14, y=0.95,
                          bbox=dict(facecolor='white', alpha=0.8, edgecolor='none'))
            ax3.axis('off')

            # ä¸“ä¸šé¢œè‰²æ¡
            cax = fig.add_subplot(gs[:, 3])
            cbar = plt.colorbar(im, cax=cax, aspect=10)
            cbar.set_label('Activation Intensity', rotation=270,
                           labelpad=20, fontsize=12)
            # ä¿å­˜ä¼˜åŒ–ç»“æœ
            output_file = os.path.join(output_path, f"{filename}_cam_optimized.png")
            plt.savefig(output_file, bbox_inches='tight', pad_inches=0.1, dpi=600)
            plt.close()

        except Exception as e:
            print(f"å¯è§†åŒ–ä¼˜åŒ–å¤±è´¥: {str(e)}")
            raise

def main():
    # ==================== é…ç½®å‚æ•° ====================
    MODEL_DIR = "/root/autodl-tmp/ResEmoteNet/runs/20250312-202814/models"
    MODEL_FILE = "best_test_model.pth"
    INPUT_PATH = "/root/autodl-tmp/ResEmoteNet/attention/test_1404_aligned_anger.jpg"
    OUTPUT_DIR = "/root/autodl-tmp/ResEmoteNet/attention/results"

    try:
        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        generator = GradCAMGenerator(MODEL_DIR, MODEL_FILE)
        os.makedirs(OUTPUT_DIR, exist_ok=True)

        # å¤„ç†è¾“å…¥è·¯å¾„
        if os.path.isfile(INPUT_PATH):
            print(f"ğŸ” å¤„ç†å•å¼ å›¾åƒ: {INPUT_PATH}")
            original_img, cam, _ = generator.generate_cam(INPUT_PATH)
            filename = os.path.splitext(os.path.basename(INPUT_PATH))[0]
            generator.visualize(original_img, cam, OUTPUT_DIR, filename)

        elif os.path.isdir(INPUT_PATH):
            print(f"ğŸ“‚ æ‰¹é‡å¤„ç†ç›®å½•: {INPUT_PATH}")
            for img_file in sorted(os.listdir(INPUT_PATH)):
                if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    img_path = os.path.join(INPUT_PATH, img_file)
                    print(f"  â¤ å¤„ç†: {img_file}")
                    original_img, cam, _ = generator.generate_cam(img_path)
                    filename = os.path.splitext(img_file)[0]
                    generator.visualize(original_img, cam, OUTPUT_DIR, filename)

        else:
            raise ValueError(f"æ— æ•ˆè¾“å…¥è·¯å¾„: {INPUT_PATH}")

    except Exception as e:
        print(f"âŒ è¿è¡Œç»ˆæ­¢: {str(e)}")
        exit(1)


if __name__ == "__main__":
    main()