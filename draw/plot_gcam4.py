import os
import cv2
import torch
import numpy as np
import matplotlib.pyplot as plt
from torchvision import transforms
from PIL import Image
import matplotlib as mpl

# ==================== å­¦æœ¯å›¾è¡¨å…¨å±€é…ç½® ====================
plt.style.use('ggplot')  # æ›´å­¦æœ¯çš„ç½‘æ ¼æ ·å¼
mpl.rcParams.update({
    'font.family': 'DejaVu Sans',  # å¼€æºæ— è¡¬çº¿å­—ä½“
    'axes.titlesize': 16,  # IEEE Trans æ¨èæ ‡é¢˜å­—å·
    'axes.labelsize': 14,  # åæ ‡è½´æ ‡ç­¾å­—å·
    'xtick.labelsize': 12,
    'ytick.labelsize': 12,
    'figure.dpi': 300,  # äº¤äº’æ˜¾ç¤ºæ¸…æ™°åº¦
    'savefig.dpi': 600,  # å°åˆ·çº§åˆ†è¾¨ç‡
    'image.cmap': 'viridis',  # è‰²ç›²å‹å¥½è‰²ç³»
    'legend.frameon': True,  # å›¾ä¾‹è¾¹æ¡†
    'figure.constrained_layout.use': True,
    'figure.figsize': (18, 6)  # æ ‡å‡†ä¸‰æ å¸ƒå±€å°ºå¯¸
})


class GradCAMGenerator:
    """é¢å‘å­¦æœ¯å‡ºç‰ˆçš„ Grad-CAM å¯è§†åŒ–å·¥å…·

    ä¸»è¦ä¼˜åŒ–ç‰¹æ€§:
    - æ”¯æŒæ‰¹é‡å¤„ç† (å•å›¾/ç›®å½•)
    - è‡ªé€‚åº”è®¾å¤‡é€‰æ‹© (CUDA/MPS/CPU)
    - å¯é…ç½®çš„å­¦æœ¯å›¾è¡¨æ ·å¼
    - å¼‚å¸¸å¤„ç†æœºåˆ¶
    """

    def __init__(self, model_dir, model_file):
        # è®¾å¤‡é…ç½®
        self.device = self._get_device()
        print(f"âš™ï¸ è¿è¡Œè®¾å¤‡: {self.device}")

        # æ¨¡å‹åˆå§‹åŒ–
        self.model = self._load_model(model_dir, model_file)
        self.hook = None

        # å›¾åƒé¢„å¤„ç†æµæ°´çº¿
        self.preprocess = transforms.Compose([
            transforms.Resize((64, 64)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
        ])

    class Hook:
        """æ¢¯åº¦æ•è·å·¥å…· (æ”¯æŒå¤šæ¨¡æ€ç‰¹å¾æå–)"""

        def __init__(self):
            self.forward_out = None
            self.backward_out = None

        def register(self, module):
            def forward_hook(_, __, output):
                self.forward_out = output.detach()

            def backward_hook(_, grad_input, grad_output):
                self.backward_out = grad_output[0].detach()

            module.register_forward_hook(forward_hook)
            module.register_full_backward_hook(backward_hook)

    def _get_device(self):
        """è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è®¡ç®—è®¾å¤‡"""
        if torch.cuda.is_available():
            return torch.device("cuda")
        elif torch.backends.mps.is_available():
            return torch.device("mps")
        else:
            return torch.device("cpu")

    def _load_model(self, model_dir, model_file):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ (å«å®Œæ•´æ€§æ ¡éªŒ)"""
        from approach.DA_EmoNet2 import DA_EmoNet  # åŒ…å« Dual_ResBlock çš„å®Œæ•´ Dual_EmoNet
        # from approach.baseline_Dual_ResBlock import Baseline_Dual_ResBlock
        model_path = os.path.join(model_dir, model_file)
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"âŒ æ¨¡å‹æ–‡ä»¶ç¼ºå¤±: {model_path}")

        print(f"âœ… æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        model = DA_EmoNet().to(self.device)
        checkpoint = torch.load(model_path, map_location=self.device)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        return model

    def _process_image(self, image_path):
        """å›¾åƒé¢„å¤„ç†æµæ°´çº¿ (å«å¼‚å¸¸å¤„ç†)"""
        try:
            img = Image.open(image_path).convert('RGB')
            original_img = np.array(img)
            img_tensor = self.preprocess(img).unsqueeze(0).to(self.device)
            return original_img, img_tensor
        except Exception as e:
            raise RuntimeError(f"å›¾åƒå¤„ç†å¤±è´¥: {image_path} | é”™è¯¯: {str(e)}")

    def generate_cam(self, image_path):
        """ç”Ÿæˆ Grad-CAM çƒ­åŠ›å›¾"""
        try:
            # æ³¨å†Œé’©å­åˆ°æœ€åä¸€ä¸ªå·ç§¯å±‚
            self.hook = self.Hook()
            self.hook.register(self.model.res_block3.attention)  # ä¿®æ”¹ä¸º conv2

            # å›¾åƒé¢„å¤„ç†
            original_img, img_tensor = self._process_image(image_path)

            # å‰å‘ä¼ æ’­
            logits = self.model(img_tensor)
            probabilities = torch.nn.functional.softmax(logits, dim=1)
            pred_class = torch.argmax(probabilities).item()

            # åå‘ä¼ æ’­
            self.model.zero_grad()
            one_hot = torch.zeros_like(logits)
            one_hot[0][pred_class] = 1
            logits.backward(gradient=one_hot, retain_graph=True)

            # è®¡ç®— CAM
            weights = torch.mean(self.hook.backward_out, dim=[2, 3], keepdim=True)
            cam = torch.sum(weights * self.hook.forward_out, dim=1, keepdim=True)
            cam = torch.relu(cam).squeeze().cpu().numpy()

            # åå¤„ç†
            cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)
            cam = cv2.resize(cam, (original_img.shape[1], original_img.shape[0]))
            return original_img, cam, pred_class

        except Exception as e:
            print(f"ğŸ”¥ Grad-CAM ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise

    def visualize(self, original_img, cam, output_path, filename):
        """å­¦æœ¯çº§å¯è§†åŒ–è¾“å‡º"""
        try:
            #
            # # è‡ªé€‚åº”æ¨¡ç³Šå‚æ•°è°ƒæ•´ï¼ˆæ ¹æ®çƒ­åŠ›å›¾å°ºå¯¸åŠ¨æ€è®¡ç®—ï¼‰
            # h, w = cam.shape
            # base_size = min(h, w)
            #
            # # æ™ºèƒ½å‚æ•°è®¡ç®—ï¼ˆç»éªŒå…¬å¼ï¼‰
            # sigma_ratio = 0.015  # åŸºç¡€æ¯”ä¾‹ç³»æ•°ï¼Œå¯è°ƒèŒƒå›´0.01-0.03
            # kernel_ratio = 0.04  # æ ¸å°ºå¯¸æ¯”ä¾‹ï¼Œå¯è°ƒèŒƒå›´0.03-0.06
            #
            # # åŠ¨æ€å‚æ•°ç”Ÿæˆ
            # sigma = max(5.0, base_size * sigma_ratio)  # ä¿è¯æœ€å°sigma=3.0
            # kernel_size = int(base_size * kernel_ratio)
            # kernel_size = kernel_size + 1 if kernel_size % 2 == 0 else kernel_size  # ä¿è¯å¥‡æ•°
            #
            # # æ‰§è¡Œè‡ªé€‚åº”é«˜æ–¯æ¨¡ç³Š
            # blurred_cam = cv2.GaussianBlur(
            #     cam,
            #     (kernel_size, kernel_size),
            #     sigmaX=sigma,
            #     sigmaY=sigma
            # )
            # # åˆ›å»ºå åŠ å›¾åƒ
            heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_VIRIDIS)
            superimposed = cv2.addWeighted(
                cv2.cvtColor(original_img, cv2.COLOR_RGB2BGR), 0.5,
                heatmap, 0.5, 0
            )

            # åˆ›å»ºç”»å¸ƒ
            fig, axes = plt.subplots(1, 3, figsize=(18, 6))
            fig.suptitle('', y=0.95, fontsize=18, weight='bold')

            # å­å›¾1: åŸå›¾
            axes[0].imshow(original_img)
            axes[0].set_title('(a) Input Image', fontsize=14, pad=10)
            axes[0].axis('off')

            # å­å›¾2: çƒ­åŠ›å›¾
            im = axes[1].imshow(cam, cmap='viridis')
            axes[1].set_title('(b) Activation Map', fontsize=14, pad=10)
            axes[1].axis('off')
            cbar = fig.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)
            cbar.set_label('Activation Intensity', rotation=270, labelpad=15)

            # å­å›¾3: å åŠ æ•ˆæœ
            axes[2].imshow(cv2.cvtColor(superimposed, cv2.COLOR_BGR2RGB))
            axes[2].set_title('(c) Overlay Result', fontsize=14, pad=10)
            axes[2].axis('off')

            # ä¿å­˜ç»“æœ
            output_file = os.path.join(output_path, f"{filename}_cam.png")
            plt.savefig(output_file, bbox_inches='tight', pad_inches=0.1, dpi=600)
            plt.close()
            print(f"ğŸ’¾ ç»“æœä¿å­˜è‡³: {output_file}")

        except Exception as e:
            print(f"ğŸ”¥ å¯è§†åŒ–ä¿å­˜å¤±è´¥: {str(e)}")
            raise

def main():
    # ==================== é…ç½®å‚æ•° ====================
    # MODEL_PATH = "/root/autodl-tmp/ResEmoteNet/runs/20250404-142614/models/baseline_Dual_ResBlock_best_test_model.pth"
    # / best_test_model
    MODEL_DIR = "/root/autodl-tmp/ResEmoteNet/runs/20250312-202814/models"
    # MODEL_DIR = "/root/autodl-tmp/ResEmoteNet/runs/20250404-142614/models"
    MODEL_FILE = "best_test_model.pth"
    INPUT_PATH = "/root/autodl-tmp/ResEmoteNet/20-attention"
    OUTPUT_DIR = "/root/autodl-tmp/ResEmoteNet/attention/results2"
    try:
        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        generator = GradCAMGenerator(MODEL_DIR, MODEL_FILE)
        os.makedirs(OUTPUT_DIR, exist_ok=True)

        # å¤„ç†è¾“å…¥è·¯å¾„
        if os.path.isfile(INPUT_PATH):
            print(f"ğŸ” å¤„ç†å•å¼ å›¾åƒ: {INPUT_PATH}")
            original_img, cam, _ = generator.generate_cam(INPUT_PATH)
            filename = os.path.splitext(os.path.basename(INPUT_PATH))[0]
            generator.visualize(original_img, cam, OUTPUT_DIR, filename)

        elif os.path.isdir(INPUT_PATH):
            print(f"ğŸ“‚ æ‰¹é‡å¤„ç†ç›®å½•: {INPUT_PATH}")
            for img_file in sorted(os.listdir(INPUT_PATH)):
                if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    img_path = os.path.join(INPUT_PATH, img_file)
                    print(f"  â¤ å¤„ç†: {img_file}")
                    original_img, cam, _ = generator.generate_cam(img_path)
                    filename = os.path.splitext(img_file)[0]
                    generator.visualize(original_img, cam, OUTPUT_DIR, filename)

        else:
            raise ValueError(f"æ— æ•ˆè¾“å…¥è·¯å¾„: {INPUT_PATH}")

    except Exception as e:
        print(f"âŒ è¿è¡Œç»ˆæ­¢: {str(e)}")
        exit(1)


if __name__ == "__main__":
    main()