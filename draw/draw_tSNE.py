import os
import torch
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.manifold import TSNE
from torchvision import transforms
from sklearn.decomposition import PCA
from torch.utils.data import DataLoader
from approach.DA_EmoNet2 import DA_EmoNet  # åŒ…å« Dual_ResBlock çš„å®Œæ•´ Dual_EmoNet
from approach.baseline_res18 import ResNet18, ResNet34

from get_dataset import Four4All

# ==================== å­¦æœ¯å¯è§†åŒ–é…ç½® ====================
plt.style.use('ggplot')
plt.rcParams.update({
    'font.family': 'DejaVu Sans',
    'axes.titlesize': 16,
    'axes.labelsize': 14,
    'xtick.labelsize': 12,
    'ytick.labelsize': 12,
    'legend.fontsize': 12,
    'figure.dpi': 300,
    'savefig.format': 'svg',
    'svg.fonttype': 'none'  # ç¡®ä¿æ–‡å­—å¯ç¼–è¾‘
})


class TSNEVisualizer:
    def __init__(self, model_path, num_classes=7):
        # è®¾å¤‡é…ç½®
        self.device = torch.device(
            "cuda" if torch.cuda.is_available() else
            "mps" if torch.backends.mps.is_available() else "cpu"
        )

        # æ¨¡å‹åŠ è½½
        self.model = self._load_model(model_path)
        self.num_classes = num_classes

        # æ•°æ®è½¬æ¢
        self.transform = transforms.Compose([
            transforms.Resize((64, 64)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
        ])

    def _load_model(self, model_path):
        """å®‰å…¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

        print(f"âœ… æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        model = ResNet18().to(self.device)
        checkpoint = torch.load(model_path, map_location=self.device)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        return model

    def _get_features(self, dataloader):
        """æ‰¹é‡æå–ç‰¹å¾"""
        features = []
        labels = []

        with torch.no_grad():
            for inputs, targets in dataloader:
                inputs = inputs.to(self.device)
                # å…³é”®ä¿®æ”¹ï¼šé€šè¿‡å®Œæ•´æ¨¡å‹å‰å‘ä¼ æ’­
                outputs = self.model(inputs)  # å‡è®¾æ¨¡å‹å¯ä»¥ç›´æ¥è¿”å›ç‰¹å¾

                # å¦‚æœæ¨¡å‹è¿”å›çš„æ˜¯å…ƒç»„ï¼Œéœ€è¦ç´¢å¼•ç‰¹å¾éƒ¨åˆ†
                if isinstance(outputs, tuple):
                    feats = outputs[0]  # å‡è®¾ç‰¹å¾åœ¨ç¬¬ä¸€ä¸ªè¿”å›å€¼
                else:
                    feats = outputs

                features.append(feats.cpu().numpy())
                labels.append(targets.numpy())

        return np.concatenate(features), np.concatenate(labels)

    def visualize(self, test_dataset, save_dir, n_samples=3000, perplexity=35):

        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)

        try:
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            dataloader = DataLoader(test_dataset,
                                    batch_size=16,
                                    shuffle=True,  # ä¿è¯éšæœºé‡‡æ ·
                                    num_workers=4)

            # æå–ç‰¹å¾
            print("â³ æ­£åœ¨æå–ç‰¹å¾...")
            features, labels = self._get_features(dataloader)
            print(f"æå–ç‰¹å¾æ•°: {features.shape[0]}, ç‰¹å¾ç»´åº¦: {features.shape[1]}")
            # éšæœºé‡‡æ ·
            if len(features) > n_samples:
                indices = np.random.choice(len(features), n_samples, replace=False)
                features = features[indices]
                labels = labels[indices]
            # åœ¨visualize()æ–¹æ³•ä¸­æ·»åŠ ï¼š
            print("ğŸ“Š ç‰¹å¾çŸ©é˜µç»´åº¦:", features.shape)
            # PCAé¢„é™ç»´åŠ é€Ÿ
            print("ğŸ”§ æ­£åœ¨æ‰§è¡ŒPCAé™ç»´...")
            # ä¿®æ”¹PCAåˆå§‹åŒ–ä»£ç ä¸º
            pca = PCA(n_components=min(50, features.shape[1] - 1))  # è‡ªåŠ¨é€‚é…æœ€å¤§å¯ç”¨ç»´åº¦
            features_pca = pca.fit_transform(features)

            # t-SNEé™ç»´
            print("ğŸŒ€ æ­£åœ¨æ‰§è¡Œt-SNE...")
            tsne = TSNE(n_components=2,
                        perplexity=perplexity,
                        learning_rate=200,
                        random_state=42)
            tsne_results = tsne.fit_transform(features_pca)

            # å¯è§†åŒ–
            print("ğŸ¨ æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–...")
            plt.figure(figsize=(14, 10))

            # å®šä¹‰é¢œè‰²å’Œæ ‡ç­¾
            emotion_names = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']
            colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00', '#ffff33', '#a65628']

            # ç»˜åˆ¶æ•£ç‚¹
            scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1],
                                  c=labels, cmap=ListedColormap(colors),
                                  alpha=0.8, edgecolors='k', linewidth=0.3,
                                  s=60, zorder=2)

            # æ·»åŠ å›¾ä¾‹
            legend_elements = [plt.Line2D([0], [0], marker='o', color='w',
                                          label=emotion_names[i],
                                          markerfacecolor=colors[i],
                                          markersize=12, markeredgecolor='k')
                               for i in range(7)]
            plt.legend(handles=legend_elements,
                       title='Emotion Classes',
                       bbox_to_anchor=(1.05, 1),
                       loc='upper left',
                       borderaxespad=0.,
                       frameon=False)

            # åæ ‡è½´ä¼˜åŒ–
            plt.xlabel('t-SNE Dimension 1', labelpad=12, fontweight='bold')
            plt.ylabel('t-SNE Dimension 2', labelpad=12, fontweight='bold')
            plt.title('Feature Embedding Visualization (Test Set)',
                      pad=20, fontsize=18, fontweight='bold')

            # ç½‘æ ¼çº¿å¢å¼ºå¯è¯»æ€§
            plt.grid(True, linestyle='--', alpha=0.6, zorder=1)
            # ä¿å­˜ç»“æœ
            # åŒæ ¼å¼ä¿å­˜
            base_path = os.path.join(save_dir, 'tsne_visualization')
            plt.savefig(f"{base_path}.png", bbox_inches='tight')
            plt.savefig(base_path, bbox_inches='tight')
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {base_path}")
            plt.close()

        except Exception as e:
            print(f"âŒ å¯è§†åŒ–å¤±è´¥: {str(e)}")
            raise


if __name__ == "__main__":
    # ==================== é…ç½®å‚æ•° ====================
    # MODEL_PATH = "/root/autodl-tmp/ResEmoteNet/runs/20250312-202814/models/best_test_model.pth"
    MODEL_PATH = "/root/autodl-tmp/ResEmoteNet/runs/20250404-142614/models/baseline_res18_no_aug_best_test_model.pth"
    TEST_CSV = "/root/autodl-tmp/ResEmoteNet/rafdb_tr/test_labels.csv"
    TEST_IMG_DIR = "/root/autodl-tmp/ResEmoteNet/rafdb_tr/test"
    models_dir = os.path.dirname(MODEL_PATH)
    base_dir = os.path.dirname(models_dir)
    SAVE_DIR = os.path.join(base_dir, "t-SNE")

    # åˆå§‹åŒ–å¯è§†åŒ–å™¨
    visualizer = TSNEVisualizer(MODEL_PATH)

    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    test_dataset = Four4All(
        csv_file=TEST_CSV,
        img_dir=TEST_IMG_DIR,
        transform=visualizer.transform
    )

    # æ‰§è¡Œå¯è§†åŒ–
    visualizer.visualize(
        test_dataset=test_dataset,
        save_dir=SAVE_DIR,
        n_samples=3000,  # å»ºè®®é‡‡æ ·é‡
        perplexity=35  # æ ¹æ®æ•°æ®åˆ†å¸ƒè°ƒæ•´
    )